---
services:
  postgresdb:
    container_name: postgresdb
    image: postgres:16
    profiles:
      - db
      - postgres
    ports:
      - 5432:5432
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "postgres" ]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always
    volumes:
      -  ${PWD:-.}/src/docker/pg_init.sql:/docker-entrypoint-initdb.d/pg_init.sql

  pgadmin:
    container_name: pgadmin
    image: dpage/pgadmin4
    profiles:
      - postgres
    ports:
      - "${PGADMIN_PORT:-5050}:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-pgadmin4@pgadmin.org}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: 'False'
    # volumes:
    #   - ${PWD:-.}/src/postgres/pgadmin:/var/lib/pgadmin
    restart: unless-stopped

  # doc: https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts
  # Spark Standalone: https://spark.apache.org/docs/latest/spark-standalone.html
  spark-node:
    container_name: spark-node
    profiles:
      - spark
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: da-spark-image
    ports:
      - '9090:8080'
      - '7077:7077'
    environment:
      - SPARK_MODE=master
      # Control whether the spark processes will run as daemons or not
      # Turning off else the containers will shutdown after the entrypoint script is executed.
      - SPARK_NO_DAEMONIZE=true
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 3s
      timeout: 3s
      retries: 3
    # env_file:
    #   - spark/.env.spark
    volumes:
      - ${PWD:-.}/src/spark:/opt/spark/workdir
      - ${PWD:-.}/src/spark/logs:/opt/spark/spark-events
      # - ${PWD:-.}/conf:/opt/spark/conf

  spark-test:
    container_name: spark-test
    profiles:
      - sparktest
    build:
      context: .
      dockerfile: Dockerfile.sparktest
    image: spark-test:3.5.0
    environment:
      - SPARK_MODE=master
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 3s
      timeout: 3s
      retries: 3
    volumes:
      - ${PROJECT_DIR:-${PWD}}/tests/spark:/opt/bitnami/spark/tests
      - ${PROJECT_DIR:-${PWD}}/src/spark/app:/opt/bitnami/spark/app
      # - ${PWD:-.}/conf:/opt/spark/conf
    command: python -m pytest --cache-clear -rsxX -l --tb=short --strict -v tests/  
volumes:
  spark-logs:
